{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOC.05 - ADP GETS'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Import Azure libraries for document analysis\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from typing import Optional\n",
    "\n",
    "# Import custom utility modules\n",
    "from utils.pdfparser import DocumentAnalysisPdfParser\n",
    "from utils.textsplitter import TextSplitter, SplitPage\n",
    "\n",
    "report_name = os.getenv(\"REPORT_NAME\")\n",
    "report_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from azure.core.credentials import AzureKeyCredential\n",
    "# from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "# from azure.search.documents.indexes.models import (\n",
    "#     SearchIndex,\n",
    "#     SearchField,\n",
    "#     SearchFieldDataType,\n",
    "#     SimpleField,\n",
    "#     SearchableField,\n",
    "#     VectorSearch,\n",
    "#     VectorSearchProfile,\n",
    "#     HnswAlgorithmConfiguration,\n",
    "#     SemanticConfiguration,\n",
    "#     SemanticField,\n",
    "#     SemanticPrioritizedFields,\n",
    "#     SemanticSearch,\n",
    "# )\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "# index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "# key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "\n",
    "\n",
    "# def get_index(name: str):\n",
    "#     fields = [\n",
    "#         SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "#         SearchableField(\n",
    "#             name=\"content\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             sortable=True,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SearchField(\n",
    "#             name=\"embedding\",\n",
    "#             type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "#             searchable=True,\n",
    "#             vector_search_dimensions=1536,\n",
    "#             vector_search_profile_name=\"my-vector-config\",\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"category\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"sourcepage\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"sourcefile\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"pdf_page_num\",\n",
    "#             type=SearchFieldDataType.Int32,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"section\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#     ]\n",
    "#     vector_search = VectorSearch(\n",
    "#         profiles=[\n",
    "#             VectorSearchProfile(\n",
    "#                 name=\"my-vector-config\",\n",
    "#                 algorithm_configuration_name=\"my-algorithms-config\",\n",
    "#             )\n",
    "#         ],\n",
    "#         algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
    "#     )\n",
    "\n",
    "#     semantic_config = SemanticConfiguration(\n",
    "#         name=\"my-semantic-config\",\n",
    "#         prioritized_fields=SemanticPrioritizedFields(\n",
    "#             title_field=None,\n",
    "#             content_fields=[SemanticField(field_name=\"content\")],\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     # Create the semantic settings with the configuration\n",
    "#     semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "#     return SearchIndex(\n",
    "#         name=name,\n",
    "#         fields=fields,\n",
    "#         vector_search=vector_search,\n",
    "#         semantic_search=semantic_search,\n",
    "#     )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     credential = AzureKeyCredential(key)\n",
    "#     index_client = SearchIndexClient(service_endpoint, credential)\n",
    "\n",
    "#     # Delete Index\n",
    "#     index_client.delete_index(index_name)\n",
    "\n",
    "#     # Create Index\n",
    "#     index = get_index(index_name)\n",
    "#     index_client.create_or_update_index(index)\n",
    "\n",
    "#     print(\"Created Index\",f\"{index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 100\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(soc_report_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pdf_file:\n\u001b[0;32m     96\u001b[0m     poller \u001b[38;5;241m=\u001b[39m document_analysis_client\u001b[38;5;241m.\u001b[39mbegin_analyze_document(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprebuilt-layout\u001b[39m\u001b[38;5;124m\"\u001b[39m, document\u001b[38;5;241m=\u001b[39mpdf_file\n\u001b[0;32m     98\u001b[0m     )\n\u001b[1;32m--> 100\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpoller\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     documents \u001b[38;5;241m=\u001b[39m create_documents(result, pdf_file)\n\u001b[0;32m    104\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-chunks.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\TH967NM\\AppData\\Local\\anaconda3\\envs\\Langgraph\\Lib\\site-packages\\azure\\core\\polling\\_poller.py:254\u001b[0m, in \u001b[0;36mLROPoller.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PollingReturnType_co:\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the result of the long running operation, or\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03m    the result available after the specified timeout.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    :raises ~azure.core.exceptions.HttpResponseError: Server problem with the query.\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_polling_method\u001b[38;5;241m.\u001b[39mresource()\n",
      "File \u001b[1;32mc:\\Users\\TH967NM\\AppData\\Local\\anaconda3\\envs\\Langgraph\\Lib\\site-packages\\azure\\core\\tracing\\decorator.py:105\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[1;32mc:\\Users\\TH967NM\\AppData\\Local\\anaconda3\\envs\\Langgraph\\Lib\\site-packages\\azure\\core\\polling\\_poller.py:269\u001b[0m, in \u001b[0;36mLROPoller.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# Let's handle possible None in forgiveness here\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# https://github.com/python/mypy/issues/8165\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\TH967NM\\AppData\\Local\\anaconda3\\envs\\Langgraph\\Lib\\threading.py:1126\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1126\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\TH967NM\\AppData\\Local\\anaconda3\\envs\\Langgraph\\Lib\\threading.py:1146\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1147\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from typing import Optional\n",
    "\n",
    "from utils.pdfparser import DocumentAnalysisPdfParser\n",
    "from utils.textsplitter import TextSplitter, SplitPage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "credential = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "report_name = os.getenv(\"REPORT_NAME\")\n",
    "\n",
    "\n",
    "class Section:\n",
    "    \"\"\"\n",
    "    A section of a page that is stored in a search service. These sections are used as context by Azure OpenAI service\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split_page: SplitPage,\n",
    "        content: str,\n",
    "        pdf_page_no: int,\n",
    "        section: str,\n",
    "        category: Optional[str] = None,\n",
    "    ):\n",
    "        self.split_page = split_page\n",
    "        self.content = content\n",
    "        self.category = category\n",
    "        self.pdf_page_no = pdf_page_no\n",
    "        self.section = section\n",
    "\n",
    "\n",
    "def create_documents(result, pdf_file):\n",
    "\n",
    "    pdf_parser = DocumentAnalysisPdfParser(endpoint=endpoint, credential=credential)\n",
    "    text_splitter = TextSplitter(has_image_embeddings=False)\n",
    "\n",
    "    pages = [page for page in pdf_parser.parse(result=result)]\n",
    "\n",
    "    sections = [\n",
    "        Section(\n",
    "            split_page=split_page,\n",
    "            content=pdf_file,\n",
    "            category=None,\n",
    "            pdf_page_no=split_page.pdf_page_num,\n",
    "            section=split_page.section,\n",
    "        )\n",
    "        for split_page in text_splitter.split_pages(pages)\n",
    "    ]\n",
    "\n",
    "    MAX_BATCH_SIZE = 1000\n",
    "    section_batches = [\n",
    "        sections[i : i + MAX_BATCH_SIZE]\n",
    "        for i in range(0, len(sections), MAX_BATCH_SIZE)\n",
    "    ]\n",
    "\n",
    "    for batch_index, batch in enumerate(section_batches):\n",
    "        documents = [\n",
    "            {\n",
    "                \"id\": f\"file-{re.sub(\"[^0-9a-zA-Z_-]\", \"_\", report_name)}-{base64.b16encode(report_name.encode(\"utf-8\")).decode(\"ascii\")}-page-{section_index + batch_index * MAX_BATCH_SIZE}\",\n",
    "                \"content\": section.split_page.text,\n",
    "                \"category\": section.category,\n",
    "                \"section\": section.section,\n",
    "                \"sourcepage\": f\"{report_name}#page={section.split_page.page_num+1}\",\n",
    "                \"sourcefile\": f\"{report_name}\",\n",
    "                \"pdf_page_num\": section.split_page.pdf_page_num,\n",
    "            }\n",
    "            for section_index, section in enumerate(batch)\n",
    "        ]\n",
    "\n",
    "    return documents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Construct the output folders\n",
    "    chunks_folder = os.path.join(\"data\", \"chunks\")\n",
    "\n",
    "    # Ensure the output foldesr exist\n",
    "    os.makedirs(chunks_folder, exist_ok=True)\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint, AzureKeyCredential(credential)\n",
    "    )\n",
    "\n",
    "    soc_report_path = os.path.join(\"data\", \"document\", f\"{report_name}.pdf\")\n",
    "\n",
    "    with open(soc_report_path, \"rb\") as pdf_file:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", document=pdf_file\n",
    "        )\n",
    "\n",
    "        result = poller.result()\n",
    "\n",
    "        documents = create_documents(result, pdf_file)\n",
    "\n",
    "    file_name = f\"{report_name}-chunks.txt\"\n",
    "    file_path = os.path.join(chunks_folder, file_name)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as output:\n",
    "        output.write(json.dumps(documents, indent=4))\n",
    "\n",
    "print(\"Chunks Created for Document\",f\"{report_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the Chunks to the Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC.17 - State Street ITGC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 109/109 [01:29<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to Index SOC.17 - State Street ITGC\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "report_name = os.getenv(\"REPORT_NAME\")\n",
    "\n",
    "print(report_name)\n",
    "\n",
    "def get_embeddings(text: str):\n",
    "    # There are a few ways to get embeddings. This is just one example.\n",
    "    import openai\n",
    "\n",
    "    open_ai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    open_ai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=open_ai_endpoint,\n",
    "        api_key=open_ai_key,\n",
    "        api_version=\"2023-03-15-preview\",\n",
    "    )\n",
    "    embedding = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "def get_documents():\n",
    "    chunks_folder = os.path.join(\"data\", \"chunks\")\n",
    "    file_name = f\"{report_name}-chunks.txt\"\n",
    "    file_path = os.path.join(chunks_folder, file_name)\n",
    "\n",
    "    # Load documents\n",
    "    with open(file_path) as chunks_file:\n",
    "        chunks = json.loads(chunks_file.read())\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for chunk in tqdm(chunks):\n",
    "        if chunk[\"content\"]:\n",
    "            item_dict = {\n",
    "                \"id\": chunk[\"id\"],\n",
    "                \"content\": chunk[\"content\"] if chunk[\"content\"] else \"empty page\",\n",
    "                \"embedding\": get_embeddings(chunk[\"content\"]),\n",
    "                \"category\": chunk[\"category\"],\n",
    "                \"section\": chunk[\"section\"],\n",
    "                \"sourcepage\": chunk[\"sourcepage\"],\n",
    "                \"sourcefile\": chunk[\"sourcefile\"],\n",
    "                \"pdf_page_num\": chunk[\"pdf_page_num\"],\n",
    "            }\n",
    "\n",
    "            documents.append(item_dict)\n",
    "\n",
    "    return documents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    credential = AzureKeyCredential(key)\n",
    "    client = SearchClient(service_endpoint, index_name, credential)\n",
    "    documents = get_documents()\n",
    "    client.upload_documents(documents=documents)\n",
    "    print(\"Added to Index\",f\"{report_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching/Retrieiving from the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC.17 - State Street ITGC\n",
      "The Chunks are Retrived and Saved as- SOC.17 - State Street ITGC-search.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.models import QueryType\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up environment variables for service connection and configuration\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  # Azure Search service endpoint\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  # Name of the Azure Search index\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")  # API key for authentication\n",
    "k_nearest_neighbors = 50  # Number of nearest neighbors to retrieve in the semantic search\n",
    "report_name = os.getenv(\"REPORT_NAME\")  # Name of the report to filter results\n",
    "\n",
    "print(report_name)\n",
    "\n",
    "def get_embeddings(text: str):\n",
    "    # There are a few ways to get embeddings. This is just one example.\n",
    "    import openai\n",
    "\n",
    "    open_ai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    open_ai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=open_ai_endpoint,\n",
    "        api_key=open_ai_key,\n",
    "        api_version=\"2023-03-15-preview\",\n",
    "    )\n",
    "    embedding = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "\n",
    "def semantic_query(query):\n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=get_embeddings(query),\n",
    "        k_nearest_neighbors=k_nearest_neighbors,\n",
    "        fields=\"embedding\",\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        semantic_configuration_name=\"my-semantic-config\",\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        filter=f\"sourcefile eq '{report_name}' and section eq 'Section 3'\",\n",
    "        select=[\"id\", \"sourcefile\", \"content\", \"pdf_page_num\"],\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Construct the output folder path\n",
    "    search_output_folder = os.path.join(\"data\", \"search\")\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(search_output_folder, exist_ok=True)\n",
    "\n",
    "    query = (\n",
    "        \"sap applications in scope or sap systems in scope or sap platforms in scope\"\n",
    "    )\n",
    "    results = semantic_query(query=query)\n",
    "\n",
    "    file_name = f\"{report_name}-search.txt\"\n",
    "    file_path = os.path.join(search_output_folder, file_name)\n",
    "\n",
    "    # Adding index to results and saving them\n",
    "    indexed_results = []\n",
    "    for idx, result in enumerate(results):\n",
    "        indexed_result = {\n",
    "            \"index\": idx + 1,  # Add index starting from 1\n",
    "            \"id\": result[\"id\"],\n",
    "            \"sourcefile\": result[\"sourcefile\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"pdf_page_num\": result[\"pdf_page_num\"],\n",
    "        }\n",
    "        indexed_results.append(indexed_result)\n",
    "\n",
    "    # Write the indexed results to the output file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as output:\n",
    "        output.write(json.dumps(indexed_results, indent=4))\n",
    "\n",
    "    print(\"The Chunks are Retrived and Saved as-\", f\"{file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC.05 - ADP GETS\n",
      "The Chunks are Retrived and Saved as- SOC.05 - ADP GETS-simple-search-update.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Importing necessary modules from Azure SDK\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.models import QueryType\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up environment variables for service connection and configuration\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  # Azure Search service endpoint\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  # Name of the Azure Search index\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")  # API key for authentication\n",
    "k_nearest_neighbors = 50  # Number of nearest neighbors to retrieve in the semantic search\n",
    "report_name = os.getenv(\"REPORT_NAME\")  # Name of the report to filter results\n",
    "\n",
    "print(report_name)\n",
    "\n",
    "# Function to get embeddings for the query text using OpenAI's API\n",
    "def get_embeddings(text: str):\n",
    "    # Importing OpenAI package to get embeddings\n",
    "    import openai\n",
    "\n",
    "    # Fetch OpenAI API endpoint and key from environment variables\n",
    "    open_ai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    open_ai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    # Initialize the OpenAI client with the necessary credentials\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=open_ai_endpoint,\n",
    "        api_key=open_ai_key,\n",
    "        api_version=\"2023-03-15-preview\",  # Specify API version\n",
    "    )\n",
    "    \n",
    "    # Request embeddings for the input text using the specified model\n",
    "    embedding = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    # Return the embedding data for the text\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "# Function to perform a semantic query on Azure Search\n",
    "def semantic_query(query):\n",
    "    # Initialize the Azure Search client with the service endpoint and credentials\n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    \n",
    "    # Create a vectorized query using the embedding generated for the input query\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=get_embeddings(query),  # The vectorized representation of the query text\n",
    "        k_nearest_neighbors=k_nearest_neighbors,  # Number of nearest neighbors to retrieve\n",
    "        fields=\"embedding\",  # Field in the index where embeddings are stored\n",
    "    )\n",
    "\n",
    "    # Perform the search operation with semantic capabilities\n",
    "    results = search_client.search(\n",
    "        query_type=QueryType.SEMANTIC,  # Specify semantic search type\n",
    "        semantic_configuration_name=\"my-semantic-config\",  # Name of the semantic configuration in Azure Search\n",
    "        search_text=query,  # The query text that is being searched\n",
    "        vector_queries=[vector_query],  # List of vector queries\n",
    "        filter=f\"sourcefile eq '{report_name}' and section eq 'Section 3'\",  # Filter results based on the report name and section\n",
    "        select=[\"id\", \"sourcefile\", \"content\", \"pdf_page_num\"],  # Fields to return from the results\n",
    "    )\n",
    "\n",
    "    # Return the results from the search query\n",
    "    return results\n",
    "\n",
    "# Main function to run the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the output folder path where search results will be saved\n",
    "    search_output_folder = os.path.join(\"data\", \"search\")\n",
    "\n",
    "    # Ensure the output folder exists; if not, it will be created\n",
    "    os.makedirs(search_output_folder, exist_ok=True)\n",
    "\n",
    "    # Define the search query for semantic search\n",
    "    # query = (\"sap applications in scope or sap systems in scope or sap platforms in scope\")\n",
    "    \n",
    "    query = (\"What services does ADP offer? What is ADP TotalSource? What types of retirement plans does ADP Retirement Services administer?\")\n",
    "    \n",
    "#     query = (\n",
    "#     \"Please extract the relevant and **unique** information from the report regarding **SAP applications, systems, and platforms**. \"\n",
    "#     \"The extracted information should specifically focus on the following:\\n\\n\"\n",
    "\n",
    "#     \"1. **Tabular Data**: Extract any tables that list or describe SAP applications, systems, or platforms. \"\n",
    "#     \"Include key details such as their names, functionalities, versions, and any other identifying attributes.\\n\\n\"\n",
    "\n",
    "#     \"2. **Bullet Points**: If the report uses bullet points to list or explain specific SAP applications, systems, or platforms, \"\n",
    "#     \"extract only those points that provide **unique** and actionable details about the systems in scope. Avoid extracting general or repetitive content.\\n\\n\"\n",
    "\n",
    "#     \"3. **Textual Information**: Extract relevant textual content that mentions **specific SAP applications**, **systems**, or **platforms**. \"\n",
    "#     \"Focus on their role, functionalities, integrations, and how they are being actively implemented or used. Prioritize **unique** details \"\n",
    "#     \"that differentiate the systems from generic or introductory content.\\n\\n\"\n",
    "\n",
    "#     \"**Additional Instructions**:\\n\"\n",
    "#     \"- Ensure the information is **unique** and not redundant.\\n\"\n",
    "#     \"- Focus only on **SAP systems actively in scope**.\\n\"\n",
    "# )\n",
    "\n",
    "    # Call the semantic query function to get search results\n",
    "    results = semantic_query(query=query)\n",
    "\n",
    "    # Define the output file name and path where the results will be saved\n",
    "    # file_name = f\"{report_name}-search.txt\"\n",
    "\n",
    "    file_name = f\"{report_name}-simple-search-update.txt\"\n",
    "\n",
    "    file_path = os.path.join(search_output_folder, file_name)\n",
    "\n",
    "    # Open the output file in write mode and save the search results with an index number\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as output:\n",
    "        indexed_results = []\n",
    "        \n",
    "        # Iterate through the results and add an index number to each result\n",
    "        for idx, result in enumerate(results):\n",
    "            indexed_result = {\n",
    "                \"index\": idx + 1,  # Add index starting from 1\n",
    "                \"id\": result[\"id\"],\n",
    "                \"sourcefile\": result[\"sourcefile\"],\n",
    "                \"content\": result[\"content\"],\n",
    "                \"pdf_page_num\": result[\"pdf_page_num\"],\n",
    "            }\n",
    "            indexed_results.append(indexed_result)\n",
    "\n",
    "        # Save the results as pretty-printed JSON\n",
    "        output.write(json.dumps(indexed_results, indent=4))  # Save indexed results in JSON format\n",
    "    \n",
    "    print(\"The Chunks are Retrived and Saved as-\", f\"{file_name}\")  # Indicate that the results have been saved successfully"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
