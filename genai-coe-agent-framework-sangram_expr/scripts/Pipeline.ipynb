{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SOC.05 - ADP GETS'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Import Azure libraries for document analysis\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from typing import Optional\n",
    "\n",
    "# Import custom utility modules\n",
    "from utils.pdfparser import DocumentAnalysisPdfParser\n",
    "from utils.textsplitter import TextSplitter, SplitPage\n",
    "\n",
    "report_name = os.getenv(\"REPORT_NAME\")\n",
    "report_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from azure.core.credentials import AzureKeyCredential\n",
    "# from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "# from azure.search.documents.indexes.models import (\n",
    "#     SearchIndex,\n",
    "#     SearchField,\n",
    "#     SearchFieldDataType,\n",
    "#     SimpleField,\n",
    "#     SearchableField,\n",
    "#     VectorSearch,\n",
    "#     VectorSearchProfile,\n",
    "#     HnswAlgorithmConfiguration,\n",
    "#     SemanticConfiguration,\n",
    "#     SemanticField,\n",
    "#     SemanticPrioritizedFields,\n",
    "#     SemanticSearch,\n",
    "# )\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "# load_dotenv()\n",
    "\n",
    "# service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "# index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "# key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "\n",
    "\n",
    "# def get_index(name: str):\n",
    "#     fields = [\n",
    "#         SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "#         SearchableField(\n",
    "#             name=\"content\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             sortable=True,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SearchField(\n",
    "#             name=\"embedding\",\n",
    "#             type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "#             searchable=True,\n",
    "#             vector_search_dimensions=1536,\n",
    "#             vector_search_profile_name=\"my-vector-config\",\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"category\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"sourcepage\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"sourcefile\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"pdf_page_num\",\n",
    "#             type=SearchFieldDataType.Int32,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#         SimpleField(\n",
    "#             name=\"section\",\n",
    "#             type=SearchFieldDataType.String,\n",
    "#             filterable=True,\n",
    "#             facetable=True,\n",
    "#         ),\n",
    "#     ]\n",
    "#     vector_search = VectorSearch(\n",
    "#         profiles=[\n",
    "#             VectorSearchProfile(\n",
    "#                 name=\"my-vector-config\",\n",
    "#                 algorithm_configuration_name=\"my-algorithms-config\",\n",
    "#             )\n",
    "#         ],\n",
    "#         algorithms=[HnswAlgorithmConfiguration(name=\"my-algorithms-config\")],\n",
    "#     )\n",
    "\n",
    "#     semantic_config = SemanticConfiguration(\n",
    "#         name=\"my-semantic-config\",\n",
    "#         prioritized_fields=SemanticPrioritizedFields(\n",
    "#             title_field=None,\n",
    "#             content_fields=[SemanticField(field_name=\"content\")],\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "#     # Create the semantic settings with the configuration\n",
    "#     semantic_search = SemanticSearch(configurations=[semantic_config])\n",
    "\n",
    "#     return SearchIndex(\n",
    "#         name=name,\n",
    "#         fields=fields,\n",
    "#         vector_search=vector_search,\n",
    "#         semantic_search=semantic_search,\n",
    "#     )\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     credential = AzureKeyCredential(key)\n",
    "#     index_client = SearchIndexClient(service_endpoint, credential)\n",
    "\n",
    "#     # Delete Index\n",
    "#     index_client.delete_index(index_name)\n",
    "\n",
    "#     # Create Index\n",
    "#     index = get_index(index_name)\n",
    "#     index_client.create_or_update_index(index)\n",
    "\n",
    "#     print(\"Created Index\",f\"{index_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chunk the Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks Created for Document SOC.05 - ADP GETS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import base64\n",
    "import json\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "from typing import Optional\n",
    "\n",
    "from utils.pdfparser import DocumentAnalysisPdfParser\n",
    "from utils.textsplitter import TextSplitter, SplitPage\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "credential = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_KEY\")\n",
    "report_name = os.getenv(\"REPORT_NAME\")\n",
    "\n",
    "class Section:\n",
    "    \"\"\"\n",
    "    A section of a page that is stored in a search service. These sections are used as context by Azure OpenAI service\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split_page: SplitPage,\n",
    "        content: str,\n",
    "        pdf_page_no: int,\n",
    "        section: str,\n",
    "        category: Optional[str] = None,\n",
    "    ):\n",
    "        self.split_page = split_page\n",
    "        self.content = content\n",
    "        self.category = category\n",
    "        self.pdf_page_no = pdf_page_no\n",
    "        self.section = section\n",
    "\n",
    "def create_documents(result, pdf_file):\n",
    "\n",
    "    pdf_parser = DocumentAnalysisPdfParser(endpoint=endpoint, credential=credential)\n",
    "    text_splitter = TextSplitter(has_image_embeddings=False)\n",
    "\n",
    "    pages = [page for page in pdf_parser.parse(result=result)]\n",
    "\n",
    "    sections = [\n",
    "        Section(\n",
    "            split_page=split_page,\n",
    "            content=pdf_file,\n",
    "            category=None,\n",
    "            pdf_page_no=split_page.pdf_page_num,\n",
    "            section=split_page.section,\n",
    "        )\n",
    "        for split_page in text_splitter.split_pages(pages)\n",
    "    ]\n",
    "\n",
    "    MAX_BATCH_SIZE = 1000\n",
    "    section_batches = [\n",
    "        sections[i : i + MAX_BATCH_SIZE]\n",
    "        for i in range(0, len(sections), MAX_BATCH_SIZE)\n",
    "    ]\n",
    "\n",
    "    for batch_index, batch in enumerate(section_batches):\n",
    "        documents = [\n",
    "            {\n",
    "                \"id\": f\"file-{re.sub(\"[^0-9a-zA-Z_-]\", \"_\", report_name)}-{base64.b16encode(report_name.encode(\"utf-8\")).decode(\"ascii\")}-page-{section_index + batch_index * MAX_BATCH_SIZE}\",\n",
    "                \"content\": section.split_page.text,\n",
    "                \"category\": section.category,\n",
    "                \"section\": section.section,\n",
    "                \"sourcepage\": f\"{report_name}#page={section.split_page.page_num+1}\",\n",
    "                \"sourcefile\": f\"{report_name}\",\n",
    "                \"pdf_page_num\": section.split_page.pdf_page_num,\n",
    "            }\n",
    "            for section_index, section in enumerate(batch)\n",
    "        ]\n",
    "\n",
    "    return documents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Construct the output folders\n",
    "    chunks_folder = os.path.join(\"data\", \"chunks\")\n",
    "\n",
    "    # Ensure the output foldesr exist\n",
    "    os.makedirs(chunks_folder, exist_ok=True)\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint, AzureKeyCredential(credential)\n",
    "    )\n",
    "\n",
    "    soc_report_path = os.path.join(\"data\", \"document\", f\"{report_name}.pdf\")\n",
    "\n",
    "    with open(soc_report_path, \"rb\") as pdf_file:\n",
    "        poller = document_analysis_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", document=pdf_file\n",
    "        )\n",
    "\n",
    "        result = poller.result()\n",
    "\n",
    "        documents = create_documents(result, pdf_file)\n",
    "\n",
    "    file_name = f\"{report_name}-chunks.txt\"\n",
    "    file_path = os.path.join(chunks_folder, file_name)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as output:\n",
    "        output.write(json.dumps(documents, indent=4))\n",
    "\n",
    "print(\"Chunks Created for Document\",f\"{report_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the Chunks to the Vector Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC.05 - ADP GETS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 74/74 [01:09<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added to Index SOC.05 - ADP GETS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")\n",
    "report_name = os.getenv(\"REPORT_NAME\")\n",
    "\n",
    "print(report_name)\n",
    "\n",
    "def get_embeddings(text: str):\n",
    "    # There are a few ways to get embeddings. This is just one example.\n",
    "    import openai\n",
    "\n",
    "    open_ai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    open_ai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=open_ai_endpoint,\n",
    "        api_key=open_ai_key,\n",
    "        api_version=\"2023-03-15-preview\",\n",
    "    )\n",
    "    embedding = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "def get_documents():\n",
    "    chunks_folder = os.path.join(\"data\", \"chunks\")\n",
    "    file_name = f\"{report_name}-chunks.txt\"\n",
    "    file_path = os.path.join(chunks_folder, file_name)\n",
    "\n",
    "    # Load documents\n",
    "    with open(file_path) as chunks_file:\n",
    "        chunks = json.loads(chunks_file.read())\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for chunk in tqdm(chunks):\n",
    "        if chunk[\"content\"]:\n",
    "            item_dict = {\n",
    "                \"id\": chunk[\"id\"],\n",
    "                \"content\": chunk[\"content\"] if chunk[\"content\"] else \"empty page\",\n",
    "                \"embedding\": get_embeddings(chunk[\"content\"]),\n",
    "                \"category\": chunk[\"category\"],\n",
    "                \"section\": chunk[\"section\"],\n",
    "                \"sourcepage\": chunk[\"sourcepage\"],\n",
    "                \"sourcefile\": chunk[\"sourcefile\"],\n",
    "                \"pdf_page_num\": chunk[\"pdf_page_num\"],\n",
    "            }\n",
    "\n",
    "            documents.append(item_dict)\n",
    "\n",
    "    return documents\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    credential = AzureKeyCredential(key)\n",
    "    client = SearchClient(service_endpoint, index_name, credential)\n",
    "    documents = get_documents()\n",
    "    client.upload_documents(documents=documents)\n",
    "    \n",
    "    print(\"Added to Index\",f\"{report_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching/Retrieiving from the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC.05 - ADP GETS\n",
      "The Chunks are Retrived and Saved as- SOC.05 - ADP GETS-search.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.models import QueryType\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up environment variables for service connection and configuration\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  # Azure Search service endpoint\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  # Name of the Azure Search index\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")  # API key for authentication\n",
    "k_nearest_neighbors = 50  # Number of nearest neighbors to retrieve in the semantic search\n",
    "report_name = os.getenv(\"REPORT_NAME\")  # Name of the report to filter results\n",
    "\n",
    "print(report_name)\n",
    "\n",
    "def get_embeddings(text: str):\n",
    "    # There are a few ways to get embeddings. This is just one example.\n",
    "    import openai\n",
    "\n",
    "    open_ai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    open_ai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=open_ai_endpoint,\n",
    "        api_key=open_ai_key,\n",
    "        api_version=\"2023-03-15-preview\",\n",
    "    )\n",
    "    embedding = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "def semantic_query(query):\n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=get_embeddings(query),\n",
    "        k_nearest_neighbors=k_nearest_neighbors,\n",
    "        fields=\"embedding\",\n",
    "    )\n",
    "\n",
    "    results = search_client.search(\n",
    "        query_type=QueryType.SEMANTIC,\n",
    "        semantic_configuration_name=\"my-semantic-config\",\n",
    "        search_text=query,\n",
    "        vector_queries=[vector_query],\n",
    "        filter=f\"sourcefile eq '{report_name}' and section eq 'Section 3'\",\n",
    "        select=[\"id\", \"sourcefile\", \"content\", \"pdf_page_num\"],\n",
    "    )\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Construct the output folder path\n",
    "    search_output_folder = os.path.join(\"data\", \"search\")\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(search_output_folder, exist_ok=True)\n",
    "\n",
    "    query = (\n",
    "        \"sap applications in scope or sap systems in scope or sap platforms in scope\"\n",
    "    )\n",
    "    results = semantic_query(query=query)\n",
    "\n",
    "    file_name = f\"{report_name}-search.txt\"\n",
    "    file_path = os.path.join(search_output_folder, file_name)\n",
    "\n",
    "    # Adding index to results and saving them\n",
    "    indexed_results = []\n",
    "    for idx, result in enumerate(results):\n",
    "        indexed_result = {\n",
    "            \"index\": idx + 1,  # Add index starting from 1\n",
    "            \"id\": result[\"id\"],\n",
    "            \"sourcefile\": result[\"sourcefile\"],\n",
    "            \"content\": result[\"content\"],\n",
    "            \"pdf_page_num\": result[\"pdf_page_num\"],\n",
    "        }\n",
    "        indexed_results.append(indexed_result)\n",
    "\n",
    "    # Write the indexed results to the output file\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as output:\n",
    "        output.write(json.dumps(indexed_results, indent=4))\n",
    "\n",
    "    print(\"The Chunks are Retrived and Saved as-\", f\"{file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOC.05 - ADP GETS\n",
      "The Chunks are Retrived and Saved as- SOC.05 - ADP GETS-simple-search-update.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Importing necessary modules from Azure SDK\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.search.documents.models import QueryType\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set up environment variables for service connection and configuration\n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\")  # Azure Search service endpoint\n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")  # Name of the Azure Search index\n",
    "key = os.getenv(\"AZURE_SEARCH_API_KEY\")  # API key for authentication\n",
    "k_nearest_neighbors = 50  # Number of nearest neighbors to retrieve in the semantic search\n",
    "report_name = os.getenv(\"REPORT_NAME\")  # Name of the report to filter results\n",
    "\n",
    "print(report_name)\n",
    "\n",
    "# Function to get embeddings for the query text using OpenAI's API\n",
    "def get_embeddings(text: str):\n",
    "    # Importing OpenAI package to get embeddings\n",
    "    import openai\n",
    "\n",
    "    # Fetch OpenAI API endpoint and key from environment variables\n",
    "    open_ai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    open_ai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "    # Initialize the OpenAI client with the necessary credentials\n",
    "    client = openai.AzureOpenAI(\n",
    "        azure_endpoint=open_ai_endpoint,\n",
    "        api_key=open_ai_key,\n",
    "        api_version=\"2023-03-15-preview\",  # Specify API version\n",
    "    )\n",
    "    \n",
    "    # Request embeddings for the input text using the specified model\n",
    "    embedding = client.embeddings.create(input=[text], model=\"text-embedding-ada-002\")\n",
    "    # Return the embedding data for the text\n",
    "    return embedding.data[0].embedding\n",
    "\n",
    "# Function to perform a semantic query on Azure Search\n",
    "def semantic_query(query):\n",
    "    # Initialize the Azure Search client with the service endpoint and credentials\n",
    "    search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))\n",
    "    \n",
    "    # Create a vectorized query using the embedding generated for the input query\n",
    "    vector_query = VectorizedQuery(\n",
    "        vector=get_embeddings(query),  # The vectorized representation of the query text\n",
    "        k_nearest_neighbors=k_nearest_neighbors,  # Number of nearest neighbors to retrieve\n",
    "        fields=\"embedding\",  # Field in the index where embeddings are stored\n",
    "    )\n",
    "\n",
    "    # Perform the search operation with semantic capabilities\n",
    "    results = search_client.search(\n",
    "        query_type=QueryType.SEMANTIC,  # Specify semantic search type\n",
    "        semantic_configuration_name=\"my-semantic-config\",  # Name of the semantic configuration in Azure Search\n",
    "        search_text=query,  # The query text that is being searched\n",
    "        vector_queries=[vector_query],  # List of vector queries\n",
    "        filter=f\"sourcefile eq '{report_name}' and section eq 'Section 3'\",  # Filter results based on the report name and section\n",
    "        select=[\"id\", \"sourcefile\", \"content\", \"pdf_page_num\"],  # Fields to return from the results\n",
    "    )\n",
    "\n",
    "    # Return the results from the search query\n",
    "    return results\n",
    "\n",
    "# Main function to run the script\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the output folder path where search results will be saved\n",
    "    search_output_folder = os.path.join(\"data\", \"search\")\n",
    "\n",
    "    # Ensure the output folder exists; if not, it will be created\n",
    "    os.makedirs(search_output_folder, exist_ok=True)\n",
    "\n",
    "    # Define the search query for semantic search\n",
    "    # query = (\"sap applications in scope or sap systems in scope or sap platforms in scope\")\n",
    "    \n",
    "    query = (\"What services does ADP offer? What is ADP TotalSource? What types of retirement plans does ADP Retirement Services administer?\")\n",
    "    \n",
    "#     query = (\n",
    "#     \"Please extract the relevant and **unique** information from the report regarding **SAP applications, systems, and platforms**. \"\n",
    "#     \"The extracted information should specifically focus on the following:\\n\\n\"\n",
    "\n",
    "#     \"1. **Tabular Data**: Extract any tables that list or describe SAP applications, systems, or platforms. \"\n",
    "#     \"Include key details such as their names, functionalities, versions, and any other identifying attributes.\\n\\n\"\n",
    "\n",
    "#     \"2. **Bullet Points**: If the report uses bullet points to list or explain specific SAP applications, systems, or platforms, \"\n",
    "#     \"extract only those points that provide **unique** and actionable details about the systems in scope. Avoid extracting general or repetitive content.\\n\\n\"\n",
    "\n",
    "#     \"3. **Textual Information**: Extract relevant textual content that mentions **specific SAP applications**, **systems**, or **platforms**. \"\n",
    "#     \"Focus on their role, functionalities, integrations, and how they are being actively implemented or used. Prioritize **unique** details \"\n",
    "#     \"that differentiate the systems from generic or introductory content.\\n\\n\"\n",
    "\n",
    "#     \"**Additional Instructions**:\\n\"\n",
    "#     \"- Ensure the information is **unique** and not redundant.\\n\"\n",
    "#     \"- Focus only on **SAP systems actively in scope**.\\n\"\n",
    "# )\n",
    "\n",
    "    # Call the semantic query function to get search results\n",
    "    results = semantic_query(query=query)\n",
    "\n",
    "    # Define the output file name and path where the results will be saved\n",
    "    # file_name = f\"{report_name}-search.txt\"\n",
    "\n",
    "    file_name = f\"{report_name}-simple-search-update.txt\"\n",
    "\n",
    "    file_path = os.path.join(search_output_folder, file_name)\n",
    "\n",
    "    # Open the output file in write mode and save the search results with an index number\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as output:\n",
    "        indexed_results = []\n",
    "        \n",
    "        # Iterate through the results and add an index number to each result\n",
    "        for idx, result in enumerate(results):\n",
    "            indexed_result = {\n",
    "                \"index\": idx + 1,  # Add index starting from 1\n",
    "                \"id\": result[\"id\"],\n",
    "                \"sourcefile\": result[\"sourcefile\"],\n",
    "                \"content\": result[\"content\"],\n",
    "                \"pdf_page_num\": result[\"pdf_page_num\"],\n",
    "            }\n",
    "            indexed_results.append(indexed_result)\n",
    "\n",
    "        # Save the results as pretty-printed JSON\n",
    "        output.write(json.dumps(indexed_results, indent=4))  # Save indexed results in JSON format\n",
    "    \n",
    "    print(\"The Chunks are Retrived and Saved as-\", f\"{file_name}\")  \n",
    "    # Indicate that the results have been saved successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data/reports'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 42\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# List all the reports in the data/reports folder having file extension .xlsx\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     cummulative_output \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreport_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllm_output\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_carved_out\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[0;32m     40\u001b[0m     }\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m report_file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/reports\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m report_file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     44\u001b[0m             report_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(report_file_name)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data/reports'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "report_name = os.environ[\"REPORT_NAME\"]\n",
    "\n",
    "def extract_column_value_from_llm(data):\n",
    "    llm_output = []\n",
    "\n",
    "    for table_entity in data[\"extractedTagValues\"][\"TableEntities\"]:\n",
    "        for row in table_entity[\"PredictedRows\"]:\n",
    "            for column in row[\"PredictedColumns\"]:\n",
    "                if column[\"ColumnKey\"] == \"tag_IT_applications_tbl_applicationName\":\n",
    "                    llm_output.append(column[\"ColumnValue\"])\n",
    "\n",
    "    return llm_output\n",
    "\n",
    "def extract_column_value_from_report(df):\n",
    "    report_output = []\n",
    "\n",
    "    for item in df[\"IT applications, IT processes and ITGCs\"].to_list()[3:]:\n",
    "        if item == \"Insert additional rows as needed\":\n",
    "            break\n",
    "        report_output.append(item)\n",
    "\n",
    "    return report_output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # List all the reports in the data/reports folder having file extension .xlsx\n",
    "\n",
    "    cummulative_output = {\n",
    "        \"report_name\": [],\n",
    "        \"report_output\": [],\n",
    "        \"llm_output\": [],\n",
    "        \"is_carved_out\": [],\n",
    "    }\n",
    "\n",
    "    for report_file_name in os.listdir(\"data/reports\"):\n",
    "        if report_file_name.endswith(\".xlsx\"):\n",
    "            report_name = os.path.splitext(report_file_name)[0]\n",
    "\n",
    "            cummulative_output[\"report_name\"].append(report_name)\n",
    "\n",
    "            output_folder = os.path.join(\"data\", \"reports\")\n",
    "            file_name = f\"{report_name}.xlsx\"\n",
    "            file_path = os.path.join(output_folder, file_name)\n",
    "            df = pd.read_excel(file_path, sheet_name=\"IT apps, IT processes & ITGCs\")\n",
    "\n",
    "            report_output = extract_column_value_from_report(df)\n",
    "            cummulative_output[\"report_output\"].append(report_output)\n",
    "\n",
    "            output_folder = os.path.join(\"data\", \"output\")\n",
    "            file_name = f\"{report_name}-it-apps.txt\"\n",
    "            file_path = os.path.join(output_folder, file_name)\n",
    "\n",
    "            with open(file_path, \"r\") as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                llm_ouput = extract_column_value_from_llm(data)\n",
    "                cummulative_output[\"llm_output\"].append(llm_ouput)\n",
    "\n",
    "            if len(report_output) == 0:\n",
    "                cummulative_output[\"is_carved_out\"].append(True)\n",
    "            else:\n",
    "                cummulative_output[\"is_carved_out\"].append(False)\n",
    "\n",
    "    # Report coverage in percentage:\n",
    "    # Scan through each item in report_output and if it exists in llm_ouput,\n",
    "    # increment the count. Finally, divide the count by the total number of items\n",
    "    # in report_output and multiply by 100 to get the percentage coverage.\n",
    "\n",
    "    # Report hallunication in percentage:\n",
    "    # Scan through each item in llm_ouput and if it does not exist in report_output,\n",
    "    # increment the count. Finally, divide the count by the total number of items\n",
    "    # in llm_ouput and multiply by 100 to get the percentage hallunication.\n",
    "\n",
    "    header = \"Report Name    |    Coverage    |    Hallucination\"\n",
    "    print(\"-\" * len(header))\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    avg_cov = []\n",
    "    avg_hall = []\n",
    "\n",
    "    for i, report_name in enumerate(cummulative_output[\"report_name\"]):\n",
    "        local_report_output = cummulative_output[\"report_output\"][i]\n",
    "        local_llm_ouput = cummulative_output[\"llm_output\"][i]\n",
    "\n",
    "        # print(f\"{report_name} | {local_report_output} | {local_llm_ouput}\")\n",
    "\n",
    "        # Coverage\n",
    "        if len(local_report_output) == 0:\n",
    "            coverage = \"N/A (Carved out)\"\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "            for item in local_report_output:\n",
    "                if item in local_llm_ouput:\n",
    "                    count += 1\n",
    "\n",
    "            coverage = count / len(local_report_output)\n",
    "            avg_cov.append(coverage)\n",
    "\n",
    "            coverage = f\"{coverage * 100}%\"\n",
    "\n",
    "        # Hallunication\n",
    "        if len(local_llm_ouput) == 0:\n",
    "            hallunication = \"Nil\"\n",
    "        else:\n",
    "            count = 0\n",
    "\n",
    "            for item in local_llm_ouput:\n",
    "                if item not in local_report_output:\n",
    "                    count += 1\n",
    "\n",
    "            hallunication = count / len(local_llm_ouput)\n",
    "            avg_hall.append(hallunication)\n",
    "\n",
    "            hallunication = f\"{hallunication * 100}%\"\n",
    "\n",
    "        print(f\"{report_name} | {coverage} | {hallunication}\")\n",
    "\n",
    "    print(\"-\" * len(header))\n",
    "    print(f\"Average | {np.mean(avg_cov) * 100}% | {np.mean(avg_hall) * 100}%\")\n",
    "    print(\"-\" * len(header))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
